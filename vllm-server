#!/usr/bin/env bash

# Default model - can be overridden by passing model name as argument
MODEL=${1:-"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"}
PORT=${2:-8000}

echo "Starting vLLM server with model: $MODEL on port $PORT"
echo "OpenAI-compatible API will be available at http://localhost:$PORT/v1"
echo "Using V0 engine to avoid NixOS compatibility issues"
echo "Enabling CPU offloading for large models (32GB RAM + 4GB VRAM)"

# Run vllm server with the proper nix-shell environment
# Force V0 engine to avoid triton compilation issues on NixOS
# Enable CPU offloading with gpu-memory-utilization to use system RAM
export VLLM_USE_V1=0
exec nix-shell /home/xi/EVA/shell.nix --run "uv run --with vllm vllm serve \"$MODEL\" --port $PORT --max-model-len 4096 --trust-remote-code --gpu-memory-utilization 0.85 --cpu-offload-gb 28"